---
layout: page
permalink: /publications/index.html
title: Publications
---



## 2023

I'm working on it.

> ***ZeroI2V: Zero-Cost Adaptation of Pre-Trained Transformers from Image to Video.*** [[Paper]](https://arxiv.org/abs/2310.01324)[[Code]](https://github.com/leexinhao/ZeroI2V)

**Xinhao Li**, Limin Wang (under review) 

> ***InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation.*** [[Paper]](https://openreview.net/forum?id=MLBdiWu4Fw&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions))[[Code]](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) 

Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, **Xinhao Li**, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao (ICLR2024 spotlight) 

<!-- > peft4 video-text

> InternVideo v2

> video-text pretraining

> audio-video-text pretraining

> video adaptation benchmark -->